{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am gonna train a neural network to classify whether the person whose face is in an image is a man or woman. The dataset can be found in the ```data``` folder, or **you can create your own dataset** using the notebook *Data Processing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some options and imports. We will use the ```fastai``` library, which was built on top of ```PyTorch```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural net to train smoother and faster, all the image is going to be resized to the length assigned by the ```sz``` variable. The ```PATH``` variable is the folder in which the dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n",
    "sz=224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time and computation resource, I am using a pretrained neural net called [```resnet models```](https://github.com/KaimingHe/deep-residual-networks) as a starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844073206a09434eab3fbac0001aa0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.802056   25.399355  0.243056  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch=resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to increase the accuracy, I'm gonna apply ```data augmentation``` to the data. Since people's face are not normally seen upside down, we will apply only the zoom-in, horizontal reflection and focus along the side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a ```learner``` (i.e. a neural network object using the model structure ```arch``` and the dataset ```data``` we've just defined. The ```precompute``` option is set to **True**, implying that the activation of the *penultimate layer* (i.e. the layer before the last one) is precomputed. Since in the next step, we will need to train only the last layer, this precomputation helps saving time; but we will have to turn it off later for the data augmentation to take effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ConvLearner.pretrained(arch, data, precompute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use a pretrained model with 1000 classes, we need to replace the last layer with a new layer of size ```1x1``` (Since we only have 2 classes). By default, this replacement is automatically done when we start training the model. Let's train the learner with one epoch to see where we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c5864e43d545c688a4d1ea49e8cf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.692106   0.367836   0.877604  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36783618, 0.8776041567325592]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, currently, without data augmentation and by only training the last layer for 1 epoch, we have accuracy of 87%. Now, let's turn off the precomputation, so that the data can be augmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the network with data generated from data augmentation process. We also apply *stochastic gradient descent with restarts (SGDR)*, which means the learning rate is gradually decreased, but eventually be increased, so that the learner will be able to \"*jump out*\" of a minimum point in case that minimum point is to sensitive to changes (i.e. a small changes of weights cause big changes in the loss). That way, the learner will be able to find a more stable minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue the process with ```learning rate``` of ```0.01```, which is gradually decrease over a cycle, before being pushed back to the start value (0.01).\n",
    "The ```cycle_len``` parameter defines that the cycle in which ```learning rate``` is decreased is 1 epoch. Number ```3``` is the number of cycles, so since each cycle lasts 1 epoch, we will have 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af50f2e763a44f898162880b62841f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.369047   0.294012   0.903646  \n",
      "    1      0.357488   0.240393   0.895833                \n",
      "    2      0.350418   0.216709   0.919271                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21670878, 0.9192708432674408]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 3, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to train not only the last layer, but also the whole neural net. By default, all the layers, except the last one, are ```frozen```, i.e. *unlearnable*, so we need to ```unfreeze``` them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a pretrained model, the earlier layers are more general and the later layers are more customed. We, therefore, want to apply smaller learning rate for earlier layers and bigger learning rate for later layers. We can do that by setting a numpy 1-D array of the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array([1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *SGDR* is great in helping us avoiding sensitive minimum, but suppose we find a not sensitive minimum, if we keep restarting our learning rate in every epoch, we might have a harder time finding the \"sweet spot\". Hence, overtime, it makes sense to lengthening the cycles, so that the weights have more chances to be fine-tuned. We add the ```cycle_mult``` parameter equals to 2, which means the cycle length with be doubled in every cycle. That way, if we start with cycle length = 1 and have 3 cycles, we should end up with ```1 + 1*2 + 1*2*2 = 7``` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2536b63290049da832604cc4cc8cbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.476566   0.263725   0.905382  \n",
      "    1      0.416922   0.225691   0.923611                \n",
      "    2      0.373433   0.238075   0.931424                \n",
      "    3      0.342238   0.211669   0.953125                \n",
      "    4      0.315419   0.2079     0.953125                \n",
      "    5      0.277854   0.206081   0.945312                \n",
      "    6      0.251074   0.208454   0.945312                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20845367, 0.9453125]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, at test time, since our images are not always squared, there are chances that the prediction is made in an area, in which the main object is not there, which makes it wrong. We can partly avoid that issue by making sure that the prediction is taken in different areas of the same picture, and take average of these predictions as the final prediction. In practice, we use the main image and 4 random augmented versions of it to make prediction. This is called ```test-time augmentation``` or ```TTA```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final result is 95% accuracy. Not bad for a dataset with just almost 500 images and a training process of 11 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_np(probs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the results using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  3]\n",
      " [ 2 48]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
